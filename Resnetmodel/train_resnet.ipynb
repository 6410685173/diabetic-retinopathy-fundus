{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import albumentations\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "def add_path(data_dir,name):\n",
    "\n",
    "  path = os.path.join(data_dir, name + '.jpg')\n",
    "  if os.path.exists(path):\n",
    "      return path\n",
    "  path = os.path.join(data_dir, name + '.jpeg')\n",
    "  if os.path.exists(path):\n",
    "      return path\n",
    "  path = os.path.join(data_dir, name + '.JPG')\n",
    "  if os.path.exists(path):\n",
    "      return path\n",
    "  path = os.path.join(data_dir, name + '.png')\n",
    "  if os.path.exists(path):\n",
    "      return path\n",
    "\n",
    "\n",
    "def load_excel(data_dir, list_file,n_class):\n",
    "\n",
    "  image_paths = []\n",
    "  labels = []\n",
    "  df_tmp = pd.read_csv(list_file)\n",
    "  augmented_indices = {}\n",
    "  class_counts = [0]*n_class\n",
    "  for c in range(n_class):\n",
    "    class_counts[c] += len(df_tmp.loc[df_tmp[\"class\"] == c].index)\n",
    "    augmented_indices[c] = [idx for idx in df_tmp.loc[df_tmp[\"class\"] == c].index]\n",
    "\n",
    "  minority_class = min(class_counts)\n",
    "\n",
    "  for ix in range(minority_class):\n",
    "    for jx in range(len(class_counts)):\n",
    "      p = \"\"\n",
    "      image_name = df_tmp[\"image\"][augmented_indices[jx][ix]]\n",
    "      label = df_tmp[\"class\"][augmented_indices[jx][ix]]\n",
    "      \n",
    "      for i in range(len(data_dir)):\n",
    "\n",
    "        p = add_path(data_dir[i],image_name)\n",
    "        if p != None :\n",
    "          break\n",
    "\n",
    "      if p == None:\n",
    "        print(f\"Image not found for {image_name}\")\n",
    "      else:\n",
    "        image_paths.append(p)\n",
    "        labels.append(label)\n",
    "  return image_paths,labels\n",
    "\n",
    "\n",
    "class DatasetGenerator(Dataset):\n",
    "\n",
    "  def __init__(self, data_dir, list_file, transform=None, n_class=6):\n",
    "\n",
    "    image_names,labels = load_excel(data_dir, list_file,n_class)\n",
    "\n",
    "    self.image_names = image_names\n",
    "    self.classes = list(set(labels))\n",
    "    self.labels = labels\n",
    "    self.n_class = n_class\n",
    "    self.transform = transform\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "\n",
    "    image_name = self.image_names[index]\n",
    "    label = self.labels[index]\n",
    "    image = Image.open(image_name)\n",
    "\n",
    "    if self.transform is not None:\n",
    "      image = self.transform(image)\n",
    "      image  = torch.FloatTensor(image)\n",
    "\n",
    "    return image,label\n",
    "\n",
    "  def get_path(self,index):\n",
    "    return self.image_names[index]\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "     return len(self.image_names)\n",
    "\n",
    "\n",
    "class DatasetGenerator2(Dataset):\n",
    "\n",
    "  def __init__(self, data_dir, list_file, transform=None, n_class=6):\n",
    "\n",
    "    image_names,labels = load_excel(data_dir, list_file,n_class)\n",
    "\n",
    "    self.image_names = image_names\n",
    "    self.classes = list(set(labels))\n",
    "    self.labels = labels\n",
    "    self.n_class = n_class\n",
    "    self.transform = transform\n",
    "    self.CLAHE = albumentations.Compose([ albumentations.CLAHE(clip_limit=(1,4), p= 1),])\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "\n",
    "    image_name = self.image_names[index]\n",
    "    label = self.labels[index]\n",
    "    image = Image.open(image_name)\n",
    "\n",
    "\n",
    "    if self.transform is not None:\n",
    "        np_image = np.array(image)\n",
    "\n",
    "        # Apply CLAHE transformation\n",
    "        transformed_CLAHE = self.CLAHE(image=np_image)['image']\n",
    "        transformed_CLAHE = Image.fromarray(transformed_CLAHE)\n",
    "        image = self.transform(transformed_CLAHE)\n",
    "\n",
    "    h = image.shape[1]\n",
    "    w = image.shape[2]\n",
    "    part1= image[:,:(h//2),:(w//2)]\n",
    "    part2= image[:,:(h//2),(w//2):]\n",
    "    part3= image[:,(h//2):,:(w//2)]\n",
    "    part4= image[:,(h//2):,(w//2):]\n",
    "\n",
    "    return part1,part2,part3,part4,label\n",
    "\n",
    "  def get_path(self,index):\n",
    "    return self.image_names[index]\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "     return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models import Inception3, Inception_V3_Weights, resnet50, ResNet50_Weights\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "\n",
    "def conv3x3(in_: int, out: int) -> nn.Module:\n",
    "    return nn.Conv2d(in_, out, 3, padding=1)\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_: int, out: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class Interpolate(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int = None,\n",
    "        scale_factor: int = None,\n",
    "        mode: str = \"nearest\",\n",
    "        align_corners: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.interp = nn.functional.interpolate\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "        self.scale_factor = scale_factor\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.interp(\n",
    "            x,\n",
    "            size=self.size,\n",
    "            scale_factor=self.scale_factor,\n",
    "            mode=self.mode,\n",
    "            align_corners=self.align_corners,\n",
    "        )\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        middle_channels: int,\n",
    "        out_channels: int,\n",
    "        is_deconv: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        if is_deconv:\n",
    "            \"\"\"\n",
    "                Paramaters for Deconvolution were chosen to avoid artifacts, following\n",
    "                link https://distill.pub/2016/deconv-checkerboard/\n",
    "            \"\"\"\n",
    "\n",
    "            self.block = nn.Sequential(\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                nn.ConvTranspose2d(\n",
    "                    middle_channels, out_channels, kernel_size=4, stride=2, padding=1\n",
    "                ),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                Interpolate(scale_factor=2, mode=\"bilinear\"),\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                ConvRelu(middle_channels, out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ResNet50_Base(nn.Module):\n",
    "    def __init__(self, n_classes,name,is_deconv: bool = False,):\n",
    "        super(ResNet50_Base,self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.name = name\n",
    "        resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2) # use pretained weight\n",
    "\n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "\n",
    "        self.encode1 = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.bn1,\n",
    "            self.relu,\n",
    "        )\n",
    "        # encoder\n",
    "        self.encode2  = resnet.layer1\n",
    "        self.encode3 = resnet.layer2\n",
    "        self.encode4 = resnet.layer3\n",
    "        self.encode5 = resnet.layer4\n",
    "\n",
    "        self.avgpool = resnet.avgpool\n",
    "\n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        e1 = self.encode1(input)\n",
    "        e2 = self.encode2(self.maxpool(e1))\n",
    "        e3 = self.encode3(e2)\n",
    "        e4 = self.encode4(e3)\n",
    "        e5 = self.encode5(e4)\n",
    "        # register the hook\n",
    "        #h = e5.register_hook(self.activations_hook)\n",
    "        output = e5\n",
    "\n",
    "        # register the hook\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        return output\n",
    "\n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AutoRes50(nn.Module):\n",
    "    def __init__(self, n_classes,name):\n",
    "        super(AutoRes50,self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.name = name\n",
    "        resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "        #part1\n",
    "        self.base1 = ResNet50_Base(n_classes,\"base1\")\n",
    "        #part2\n",
    "        self.base2 = ResNet50_Base(n_classes,\"base2\")\n",
    "        #part3\n",
    "        self.base3 = ResNet50_Base(n_classes,\"base3\")\n",
    "        #part4\n",
    "        self.base4 = ResNet50_Base(n_classes,\"base4\")\n",
    "\n",
    "        self.avg = resnet.avgpool\n",
    "        self.fc = resnet.fc\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1000,n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, part1,part2,part3,part4):\n",
    "        encode1 = self.base1(part1)\n",
    "        encode2 = self.base2(part2)\n",
    "        encode3 = self.base3(part3)\n",
    "        encode4 = self.base4(part4)\n",
    "\n",
    "        top = torch.cat([encode1, encode2], dim=3)\n",
    "        bottom = torch.cat([encode3, encode4], dim=3)\n",
    "        combine = torch.cat([top, bottom], dim=2)\n",
    "        x = self.avg(combine)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        result = self.classifier(x)\n",
    "\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_activations_gradient(self):\n",
    "        g1 = self.base1.get_activations_gradient()\n",
    "        g2 = self.base1.get_activations_gradient()\n",
    "        g3 = self.base1.get_activations_gradient()\n",
    "        g4 = self.base1.get_activations_gradient()\n",
    "\n",
    "        top = torch.cat([g1, g2], dim=3)\n",
    "        bottom = torch.cat([g3, g4], dim=3)\n",
    "        combine_gradient = torch.cat([top, bottom], dim=2)\n",
    "        return combine_gradient\n",
    "\n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, part1,part2,part3,part4):\n",
    "        encode1 = self.base1(part1)\n",
    "        encode2 = self.base2(part2)\n",
    "        encode3 = self.base3(part3)\n",
    "        encode4 = self.base4(part4)\n",
    "\n",
    "        top = torch.cat([encode1, encode2], dim=3)\n",
    "        bottom = torch.cat([encode3, encode4], dim=3)\n",
    "        combine = torch.cat([top, bottom], dim=2)\n",
    "        return combine\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import torch.nn as nn\n",
    "import torch.autograd\n",
    "import pathlib\n",
    "import torch, torchvision\n",
    "from matplotlib import rc\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.utils import save_image\n",
    "#from DataGenerator import DatasetGenerator2\n",
    "#from model import ResNet50_DR,VGG19_DR,ResNet50_DR_V2\n",
    "#from modelauto import AutoRes50\n",
    "import matplotlib.pyplot as plt\n",
    "#from modelrexnet import ReXNetV2\n",
    "# ================================================================ #\n",
    "def train_epoch(model,dataloaders,loss_fn,loss_MSE,optimizer,device,scheduler,n_examples):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  i = 1\n",
    "  for part1,part2,part3,part4,label in tqdm(dataloaders):\n",
    "    part1 = part1.to(device)\n",
    "    part2 = part2.to(device)\n",
    "    part3 = part3.to(device)\n",
    "    part4 = part4.to(device)\n",
    "    labels = label.to(device)\n",
    "    print(labels)\n",
    "\n",
    "    result = model(part1,part2,part3,part4)\n",
    "    _, preds = torch.max(result, dim=1)\n",
    "    loss = loss_fn(result, labels)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == labels)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  scheduler.step()\n",
    "\n",
    "  return model, correct_predictions.double() / n_examples ,np.mean(losses) #\n",
    "\n",
    "# ================================================================ #\n",
    "def eval_model(model, dataloaders, loss_fn, loss_MSE, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for part1,part2,part3,part4,label in tqdm(dataloaders):\n",
    "            part1 = part1.to(device)\n",
    "            part2 = part2.to(device)\n",
    "            part3 = part3.to(device)\n",
    "            part4 = part4.to(device)\n",
    "            labels = label.to(device)\n",
    "            print(labels)\n",
    "\n",
    "            result = model(part1,part2,part3,part4)\n",
    "            _, preds = torch.max(result, dim=1)\n",
    "            loss = loss_fn(result, labels)\n",
    "        \n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "# ================================================================ #\n",
    "def checkpoint_path(filename,model_name):\n",
    "\n",
    "  checkpoint_folderpath = pathlib.Path(f'/checkpoint-Resnet-6class/{model_name}')\n",
    "  print(checkpoint_folderpath)\n",
    "  checkpoint_folderpath.mkdir(exist_ok=True,parents=True)\n",
    "  return checkpoint_folderpath/filename\n",
    "# ================================================================ #\n",
    "\n",
    "def train_model(model, dataloaders_train, dataloaders_val,  dataset_sizes_train,  dataset_sizes_val, device, n_epochs=50): # train ต่อจาก epoch ที่18\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "    loss_MSE = nn.MSELoss().to(device)\n",
    "    best_model_path = checkpoint_path('best_model_state.ckpt',\"ResNet50_DR_Auto\")\n",
    "\n",
    "    #print(model)\n",
    "    train_accuracy = []\n",
    "    train_losses = []\n",
    "    val_accuracy = []\n",
    "    val_losses = []\n",
    "\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "      print(f'Epoch {epoch }/{n_epochs}')\n",
    "      print('-' * 10)\n",
    "      model, train_acc, train_loss = train_epoch(model, dataloaders_train, loss_fn, loss_MSE,optimizer, device, scheduler,n_examples=dataset_sizes_train)\n",
    "      print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "      val_acc, val_loss = eval_model(model,dataloaders_val,loss_fn, loss_MSE, device,n_examples=dataset_sizes_val)\n",
    "      print(f'validation   loss {val_loss} accuracy {val_acc}')\n",
    "      train_accuracy.append(train_acc.item())\n",
    "      train_losses.append(train_loss)\n",
    "      val_accuracy.append(val_acc.item())\n",
    "      val_losses.append(val_loss)\n",
    "\n",
    "      torch.save(model.state_dict(), checkpoint_path('best_model_state_'+str(epoch)+'.ckpt',\"ResNet50_DR_Auto\"))\n",
    "      if val_acc> best_accuracy:\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        best_accuracy = val_acc\n",
    "    #print(f'Best val accuracy: {best_accuracy}')\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    #print(f\"train_accuracy_each_epoch {train_accuracy}\")\n",
    "    #print(f\"train_losses_each_epoch {train_losses}\")\n",
    "    #print(f\"val_accuracy_each_epoch {val_accuracy}\")\n",
    "    #print(f\"val_losses_each_epoch {val_losses}\")\n",
    "\n",
    "    plot_metrics(train_accuracy, val_accuracy, 'Accuracy')\n",
    "    plot_metrics(train_losses, val_losses, 'Loss')\n",
    "    return model\n",
    "# ================================================================ #\n",
    "\n",
    "def plot_metrics(train_metrics, val_metrics, metric_name):\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo-', label=f'Training {metric_name}')\n",
    "    plt.plot(epochs, val_metrics, 'ro-', label=f'Validation {metric_name}')\n",
    "    plt.xticks(epochs)\n",
    "    plt.title(f'Training and Validation {metric_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.savefig(metric_name)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  dir =['/content/drive/MyDrive/combinedata/content/combine',\n",
    "              '/content/drive/MyDrive/TRAIN/train',\n",
    "              '/content/drive/MyDrive/TEST',\n",
    "              '/content/drive/MyDrive/augment_fold/augment_fold1',\n",
    "              '/content/drive/MyDrive/augment_fold/augment_fold2',\n",
    "              ] #  6 class\n",
    "\n",
    " #  6 class\n",
    "\n",
    "  label_train_file ='/content/drive/MyDrive/train_folder/fold1.csv'\n",
    "  label_val_file ='/content/drive/MyDrive/train_folder/f2.csv'\n",
    "\n",
    "\n",
    "  # ================================================================ #\n",
    "  # Data augmentation\n",
    "  train_transforms = transforms.Compose([\n",
    "        transforms.Resize((448,448)),\n",
    "        transforms.ToTensor(),\n",
    "      ])\n",
    "  val_trasform = transforms.Compose([\n",
    "        transforms.Resize((448,448)),\n",
    "        transforms.ToTensor(),\n",
    "      ])\n",
    "  # ================================================================ #\n",
    "\n",
    "\n",
    "\n",
    "  Imagedataset_train = DatasetGenerator2(data_dir=dir, list_file=label_train_file,\n",
    "                             n_class= 6,transform=train_transforms)\n",
    "\n",
    "  Imagedataset_val = DatasetGenerator2(data_dir=dir, list_file=label_val_file,\n",
    "                             n_class= 6,transform=val_trasform)\n",
    "\n",
    "  dataloaders_train= torch.utils.data.DataLoader(Imagedataset_train, batch_size=4, shuffle=False, num_workers=2)\n",
    "  dataloaders_val= torch.utils.data.DataLoader(Imagedataset_val, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "  dataset_train_sizes = len(Imagedataset_train)\n",
    "  dataset_val_sizes = len(Imagedataset_val)\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  # ================================================================ #\n",
    "  print(torch.cuda.is_available())\n",
    "  print(\"Device: \", device)\n",
    "  print(f\"train size: {len(Imagedataset_train)}\")\n",
    "  print(f\"val size: {len(Imagedataset_val)}\")\n",
    "\n",
    "  model_name = 'ResNet50_DR_Auto'\n",
    "  # ================================================================ #\n",
    "  model = AutoRes50(n_classes=6,name=model_name)\n",
    "  model.to(device)\n",
    "  # ================================================================ #\n",
    "  model = train_model(model,dataloaders_train, dataloaders_val, dataset_train_sizes, dataset_val_sizes, device, n_epochs=20)\n",
    "  # ================================================================ #\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

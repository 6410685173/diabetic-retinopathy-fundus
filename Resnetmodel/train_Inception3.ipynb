{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import albumentations\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "def add_path(data_dir,name):\n",
    "\n",
    "  path = os.path.join(data_dir, name + '.jpg')\n",
    "  if os.path.exists(path):\n",
    "      return path\n",
    "  path = os.path.join(data_dir, name + '.jpeg')\n",
    "  if os.path.exists(path):\n",
    "      return path\n",
    "  path = os.path.join(data_dir, name + '.JPG')\n",
    "  if os.path.exists(path):\n",
    "      return path\n",
    "  path = os.path.join(data_dir, name + '.png')\n",
    "  if os.path.exists(path):\n",
    "      return path\n",
    "\n",
    "\n",
    "def load_excel(data_dir, list_file,n_class):    \n",
    "    \n",
    "  image_paths = []\n",
    "  labels = []\n",
    "  df_tmp = pd.read_csv(list_file)\n",
    "  augmented_indices = {}\n",
    "  class_counts = [0]*n_class\n",
    "  for c in range(n_class):\n",
    "    class_counts[c] += len(df_tmp.loc[df_tmp[\"class\"] == c].index)\n",
    "    augmented_indices[c] = [idx for idx in df_tmp.loc[df_tmp[\"class\"] == c].index]\n",
    "  print(class_counts)\n",
    "  minority_class = min(class_counts)\n",
    "  print(minority_class)\n",
    "  for ix in range(minority_class):\n",
    "    for jx in range(len(class_counts)):\n",
    "      p = \"\"\n",
    "      image_name = df_tmp[\"image\"][augmented_indices[jx][ix]]\n",
    "      label = df_tmp[\"class\"][augmented_indices[jx][ix]]\n",
    "      \n",
    "      for i in range(len(data_dir)):\n",
    "        \n",
    "        p = add_path(data_dir[i],image_name)\n",
    "        if p != None :\n",
    "          break\n",
    "        \n",
    "      if p == None:\n",
    "        print(f\"Image not found for {image_name}\")\n",
    "      else:\n",
    "        image_paths.append(p)\n",
    "        labels.append(label)\n",
    "  return image_paths,labels\n",
    "\n",
    "\n",
    "class DatasetGenerator(Dataset):\n",
    "\n",
    "  def __init__(self, data_dir, list_file, transform=None, n_class=6):\n",
    "\n",
    "    image_names,labels = load_excel(data_dir, list_file,n_class)\n",
    "\n",
    "    self.image_names = image_names\n",
    "    self.classes = list(set(labels))\n",
    "    self.labels = labels\n",
    "    self.n_class = n_class\n",
    "    self.transform = transform\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "\n",
    "    image_name = self.image_names[index]\n",
    "    label = self.labels[index]\n",
    "    image = Image.open(image_name)\n",
    "\n",
    "    if self.transform is not None:\n",
    "      image = self.transform(image)\n",
    "     \n",
    "\n",
    "    return image,label\n",
    "\n",
    "  def get_path(self,index):\n",
    "    return self.image_names[index]\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "     return len(self.image_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models import resnet50, ResNet50_Weights, Inception3, Inception_V3_Weights\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "\n",
    "    \n",
    "class inception_resnet_v2(nn.Module):\n",
    "    def __init__(self, n_classes,name,pretrained=True):\n",
    "        super(inception_resnet_v2,self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.name = name\n",
    "        \n",
    "        inception_resnet = timm.create_model('inception_resnet_v2.tf_in1k', pretrained=pretrained)\n",
    "        self.conv_inres = torch.nn.Sequential(*list(inception_resnet.children())[:-3])\n",
    "        self.global_pool =  inception_resnet.global_pool\n",
    "        self.head_drop =  inception_resnet.head_drop\n",
    "        self.classif =  inception_resnet.classif\n",
    "        \n",
    "        num_features = inception_resnet.classif.out_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features,n_classes)\n",
    "        )\n",
    "        \n",
    "        self.gradients = None\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x =  self.conv_inres(input)\n",
    "        \n",
    "        #h = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        x = self.global_pool(x)\n",
    "        x = self.head_drop(x)\n",
    "        x = self.classif(x)\n",
    "        output = self.classifier(x)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        \n",
    "        self.gradients = grad\n",
    "        \n",
    "\n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        \n",
    "        return self.gradients\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import torch.nn as nn\n",
    "import torch.autograd\n",
    "import pathlib \n",
    "import torch, torchvision\n",
    "from matplotlib import rc\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================================================================ # \n",
    "def train_epoch(model,dataloaders,loss_fn,optimizer,device,scheduler,n_examples):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  i = 1\n",
    "  for image,label in tqdm(dataloaders):\n",
    "    inputs = image.to(device)\n",
    "    labels = label.to(device)\n",
    "    print(labels)\n",
    "    \n",
    "    outputs,_ = model(inputs)   \n",
    "    \n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    correct_predictions += torch.sum(preds == labels)\n",
    "    losses.append(loss.item())    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  scheduler.step()\n",
    "\n",
    "  return model, correct_predictions.double() / n_examples ,np.mean(losses) # \n",
    "\n",
    "# ================================================================ # \n",
    "def eval_model(model, dataloaders, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  with torch.no_grad():\n",
    "    for inputs, labels in tqdm(dataloaders):\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      print(labels)\n",
    "      outputs = model(inputs)\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      loss = loss_fn(outputs, labels)\n",
    "      correct_predictions += torch.sum(preds == labels)\n",
    "      losses.append(loss.item())\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses) \n",
    "\n",
    "# ================================================================ # \n",
    "def checkpoint_path(filename,model_name):\n",
    "  \n",
    "  checkpoint_folderpath = pathlib.Path(f'D:\\CN341\\Basemodel\\Resnetmodel/checkpoint-IncecptionV3-6class/{model_name}')\n",
    "  print(checkpoint_folderpath)\n",
    "  checkpoint_folderpath.mkdir(exist_ok=True,parents=True)\n",
    "  return checkpoint_folderpath/filename\n",
    "# ================================================================ # \n",
    "\n",
    "def train_model(model, dataloaders_train, dataloaders_val,  dataset_sizes_train,  dataset_sizes_val, device, n_epochs=50): # train ต่อจาก epoch ที่18\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "    loss_MSE = nn.MSELoss().to(device)\n",
    "    best_model_path = checkpoint_path('best_model_state.ckpt',model.name)\n",
    "   \n",
    "    #print(model)\n",
    "    train_accuracy = []\n",
    "    train_losses = []\n",
    "    val_accuracy = []\n",
    "    val_losses = []\n",
    "  \n",
    "    best_accuracy = 0\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "      print(f'Epoch {epoch }/{n_epochs}')\n",
    "      print('-' * 10)\n",
    "      model, train_acc, train_loss = train_epoch(model,dataloaders_train,loss_fn,optimizer,device,scheduler,n_examples=dataset_sizes_train)\n",
    "      print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "      val_acc, val_loss = eval_model(model,dataloaders_val,loss_fn,device,n_examples=dataset_sizes_val)\n",
    "      print(f'validation   loss {val_loss} accuracy {val_acc}')\n",
    "      train_accuracy.append(train_acc.item())\n",
    "      train_losses.append(train_loss)\n",
    "      val_accuracy.append(val_acc.item())\n",
    "      val_losses.append(val_loss)\n",
    "\n",
    "      torch.save(model.state_dict(), checkpoint_path('best_model_state_'+str(epoch)+'.ckpt',model.name))\n",
    "      if val_acc> best_accuracy:\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        best_accuracy = val_acc\n",
    "    #print(f'Best val accuracy: {best_accuracy}')\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    #print(f\"train_accuracy_each_epoch {train_accuracy}\")\n",
    "    #print(f\"train_losses_each_epoch {train_losses}\")\n",
    "    #print(f\"val_accuracy_each_epoch {val_accuracy}\")\n",
    "    #print(f\"val_losses_each_epoch {val_losses}\")\n",
    "    \n",
    "    plot_metrics(train_accuracy, val_accuracy, 'Accuracy')\n",
    "    plot_metrics(train_losses, val_losses, 'Loss')\n",
    "    return model\n",
    "# ================================================================ #  \n",
    "\n",
    "def plot_metrics(train_metrics, val_metrics, metric_name):\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo-', label=f'Training {metric_name}')\n",
    "    plt.plot(epochs, val_metrics, 'ro-', label=f'Validation {metric_name}')\n",
    "    plt.xticks(epochs)\n",
    "    plt.title(f'Training and Validation {metric_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.savefig(metric_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  \n",
    "  train_dir =['D:\\CN341\\Basemodel\\Resnetmodel\\messidor2preprocess\\messidor-2\\messidor-2\\preprocess'] #  6 class\n",
    "  label_train_file ='D:\\CN341\\Basemodel\\Resnetmodel\\idrid-dataset/test_messi.csv'\n",
    "  val_dir =['D:\\CN341\\Basemodel\\Resnetmodel\\idrid-dataset\\Imagenes\\Imagenes'] #  6 class\n",
    "  label_val_file ='D:\\CN341\\Basemodel\\Resnetmodel\\idrid-dataset/train5.csv'\n",
    "  \n",
    "\n",
    "  # ================================================================ # \n",
    "  # Data augmentation and normalization for training\n",
    "  # Just normalization for validation\n",
    "  \n",
    "  train_transforms = transforms.Compose([\n",
    "        transforms.Resize(342, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "      ])\n",
    "  val_trasform = transforms.Compose([\n",
    "        transforms.Resize(342, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "      ])\n",
    "  # ================================================================ # \n",
    "\n",
    "  \n",
    "\n",
    "  Imagedataset_train = DatasetGenerator(data_dir=train_dir, list_file=label_train_file,\n",
    "                             n_class= 6,transform=train_transforms)\n",
    "\n",
    "  Imagedataset_val = DatasetGenerator(data_dir=val_dir, list_file=label_val_file,\n",
    "                             n_class= 6,transform=val_trasform)\n",
    "  \n",
    "  dataloaders_train= torch.utils.data.DataLoader(Imagedataset_train, batch_size=6, shuffle=False, num_workers=2)\n",
    "  dataloaders_val= torch.utils.data.DataLoader(Imagedataset_val, batch_size=6, shuffle=False, num_workers=2)\n",
    "\n",
    "  dataset_train_sizes = len(Imagedataset_train)\n",
    "  dataset_val_sizes = len(Imagedataset_val)\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  # ================================================================ # \n",
    "  print(torch.cuda.is_available())\n",
    "  print(\"Device: \", device)\n",
    "  print(f\"train size: {len(Imagedataset_train)}\")\n",
    "  print(f\"val size: {len(Imagedataset_val)}\")\n",
    "\n",
    "  model_name = 'IncecptionV3' \n",
    "  # ================================================================ # \n",
    "  model = IncecptionV3(n_classes=6,name=model_name)\n",
    "  model.to(device)\n",
    "  # ================================================================ # \n",
    "  model = train_model(model,dataloaders_train, dataloaders_val, dataset_train_sizes, dataset_val_sizes, device, n_epochs=20)\n",
    "  # ================================================================ # \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
